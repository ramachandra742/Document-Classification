{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook aims to build  Document classification models using Logistic regression, SVC, & Multinomial Naive bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vijay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vijay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import time\n",
    "import pickle\n",
    "import json\n",
    "import unicodedata\n",
    "import tabulate\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus.reader.api import CorpusReader\n",
    "from nltk.corpus.reader.api import CategorizedCorpusReader\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from sklearn.metrics import recall_score, f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "nltk.download('stopwords')   \n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read pickle files from corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex pattern for extracting documents, pickle files & categories \n",
    "doc_pattern = r'(?!\\.)[a-z_\\s]+/[a-f0-9]+\\.json'\n",
    "pkl_pattern = r'(?!\\.)[a-z_\\s]+/[a-f0-9]+\\.pickle'\n",
    "cat_pattern = r'([a-z_\\s]+)/.*'\n",
    "\n",
    "# Create a class PickledCorpusReader\n",
    "class PickledCorpusReader(CategorizedCorpusReader,CorpusReader):\n",
    "\n",
    "    def __init__(self, root, fileids=pkl_pattern, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the corpus reader. Categorized arguments\n",
    "        ('cat_pattern', 'cat_map', and 'cat_file') are passed\n",
    "        to the  'CategorizedCorpusReader' constructor. The remaining\n",
    "        arguments are passed to the CorpusReader constructor.\n",
    "        \"\"\"\n",
    "        # Add the default category pattern if not passed into the class\n",
    "        if not any (key.startswith('cat_') for key in kwargs.keys()):\n",
    "            kwargs['cat_pattern'] = cat_pattern\n",
    "\n",
    "        # Initialize NLP Corpus reader objects\n",
    "        CategorizedCorpusReader.__init__(self, kwargs)\n",
    "        CorpusReader.__init__(self, root, fileids)\n",
    "\n",
    "    def resolve(self, fileids, categories):\n",
    "        \"\"\"\n",
    "        Returns a list of fileids or categories depending on what is passed\n",
    "        to each internal corpus reader function.\n",
    "        \"\"\"\n",
    "        if fileids is not None and categories is not None:\n",
    "            raise ValueError (\"Specify fileids or categories, not both\")\n",
    "\n",
    "        if categories is not None:\n",
    "            return self.fileids(categories)\n",
    "        return fileids\n",
    "\n",
    "    def docs(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns the document from a pickled object for each file in corpus.\n",
    "        \"\"\"\n",
    "        #List the fileids & categories\n",
    "        fileids = self.resolve(fileids, categories)\n",
    "        # Load one document into memory at a time\n",
    "        for path, enc, fileid in self.abspaths(fileids, True, True):\n",
    "            with open(path,'rb') as f:\n",
    "                yield pickle.load(f)\n",
    "\n",
    "    def paragraphs(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns a genetator where each paragraph contains a list of sentences.\n",
    "        \"\"\"\n",
    "        for doc in self.docs(fileids, categories):\n",
    "            for paragraph in doc:\n",
    "                yield paragraph\n",
    "\n",
    "    def sentences(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns a generator where each sentence contains a list of tokens\n",
    "        \"\"\"\n",
    "        for paragraph in self.paragraphs(fileids, categories):\n",
    "            for sent in paragraph:\n",
    "                yield sent\n",
    "\n",
    "    def tokens(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns a list of tokens.\n",
    "        \"\"\"\n",
    "        for sent in self.sentences(fileids,categories):\n",
    "            for token in sent:\n",
    "                yield token\n",
    "\n",
    "    def words(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns a list of (token, tag) tuples.\n",
    "        \"\"\"\n",
    "        for token in self.tokens(fileids, categories):\n",
    "            yield token[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Vocabulary_size: 58,748 , Total_word_count : 1,624,862 \n"
     ]
    }
   ],
   "source": [
    "# Use PickledCorpusReader class to read pickled files from Corpus\n",
    "reader= PickledCorpusReader('C:\\\\Users\\\\vijay\\\\Desktop\\\\Rama\\\\Python NLP projects\\\\Document classification\\\\sample')\n",
    "words = Counter(reader.words())\n",
    "print(\" Vocabulary_size: {:,} , Total_word_count : {:,} \".format(len(words.keys()),sum(words.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset contains 12 document categories ('books', 'cinema', 'cooking', 'gaming', 'sports', 'tech', 'data_science',\\   \n",
    " 'design', 'news', 'politics', 'do_it_yourself', 'business').\n",
    "\n",
    "Each categpry contains,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "books contains 71 documents and 41438 words\n",
      "business contains 389 documents and 222182 words\n",
      "cinema contains 100 documents and 69153 words\n",
      "cooking contains 30 documents and 37854 words\n",
      "data_science contains 41 documents and 31354 words\n",
      "design contains 55 documents and 18260 words\n",
      "do_it_yourself contains 122 documents and 28050 words\n",
      "gaming contains 128 documents and 70778 words\n",
      "news contains 1159 documents and 850688 words\n",
      "politics contains 149 documents and 88853 words\n",
      "sports contains 118 documents and 68884 words\n",
      "tech contains 176 documents and 97368 words\n"
     ]
    }
   ],
   "source": [
    "for category in reader.categories():\n",
    "\n",
    "    n_docs = len(reader.fileids(categories=[category]))\n",
    "    n_words = sum(1 for word in reader.words(categories=[category]))\n",
    "\n",
    "    print(f'{category} contains {n_docs} documents and {n_words} words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the pickled files from reader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CorpusLoader class\n",
    "class CorpusLoader(object):\n",
    "    # Initialize class variables\n",
    "    def __init__(self, reader, folds=None, categories=None, shuffle=True):\n",
    "        self.reader=reader\n",
    "        self.folds=KFold(n_splits=folds,shuffle=shuffle)\n",
    "        self.files=np.asarray(self.reader.fileids(categories=categories))\n",
    "\n",
    "    # Method to access a listing of fileids by foldID.\n",
    "    def fileids(self, idx=None):\n",
    "        if idx is None:\n",
    "            return self.files\n",
    "        return self.files[idx]\n",
    "\n",
    "    # Returns a genetator with one document at a time\n",
    "    def documents(self,idx=None):\n",
    "        for fileid in self.fileids(idx):\n",
    "            yield list(self.reader.docs(fileids=[fileid]))\n",
    "\n",
    "    # To look up label from the corpus & returns a label for each document.\n",
    "    def labels(self, idx=None):\n",
    "        return [\n",
    "        self.reader.categories(fileids=[fileid])[0]\n",
    "        for fileid in self.fileids(idx)\n",
    "        ]\n",
    "\n",
    "    # Iterator to split training & test data for each fold using KFold's Split()\n",
    "    def __iter__(self):\n",
    "        for train_index, test_index in self.folds.split(self.files):\n",
    "            X_train = self.documents(train_index)\n",
    "            y_train = self.labels(train_index)\n",
    "\n",
    "            X_test = self.documents(test_index)\n",
    "            y_test = self.labels(test_index)\n",
    "\n",
    "            yield X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Document classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset has already been tokenized, so create a identity for tokenizer.\n",
    "def identity(words):\n",
    "    return words\n",
    "\n",
    "# Class for normalizing text.\n",
    "class TextNormalizer(BaseEstimator,TransformerMixin):\n",
    "\n",
    "    def __init__(self, language ='english'):\n",
    "        self.stopwords = set(stopwords.words(language))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def is_punct(self, token):\n",
    "        \"\"\"\n",
    "        Check the punctuation character.\n",
    "        \"\"\"\n",
    "        return all (\n",
    "        unicodedata.category(char).startswith('P') for char in token\n",
    "        )\n",
    "\n",
    "    def is_stopword(self,token):\n",
    "        return token.lower() in self.stopwords\n",
    "\n",
    "    def doc_normalize(self, doc):\n",
    "        \"\"\"\n",
    "        Removes stopwords and punctuation, lowercases, lemmatizes\n",
    "        \"\"\"\n",
    "        return [\n",
    "        self.lemmatize(token,tag).lower()\n",
    "        for paragraph in doc\n",
    "        for sent in paragraph\n",
    "        for (token,tag) in sent\n",
    "        if not self.is_punct(token) and not self.is_stopword(token)\n",
    "        ]\n",
    "\n",
    "    def lemmatize(self, token, pos_tag):\n",
    "        \"\"\"\n",
    "        Return the WordNet POS tag from the Penn Treebank tag\n",
    "        \"\"\"\n",
    "        tag = {\n",
    "        'N' : wn.NOUN,\n",
    "        'V' : wn.VERB,\n",
    "        'R' : wn.ADV,\n",
    "        'J' : wn.ADJ\n",
    "        }.get(pos_tag[0], wn.NOUN)\n",
    "\n",
    "        return self.lemmatizer.lemmatize(token, tag)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, documents):\n",
    "        for doc in documents:\n",
    "            yield self.doc_normalize(doc[0])\n",
    "\n",
    "# Create pipeline for all models.\n",
    "def pipeline(estimator):\n",
    "    steps=[\n",
    "    ('normalize',TextNormalizer()),\n",
    "    ('vectorize',TfidfVectorizer(\n",
    "    tokenizer = identity, preprocessor=None, lowercase=False\n",
    "    ))\n",
    "    ]\n",
    "    # Add the estimator\n",
    "    steps.append(('classifier',estimator))\n",
    "    return Pipeline(steps)\n",
    "\n",
    "labels = ['books','cinema','cooking','gaming','sports','tech','data_science',\\\n",
    "            'design','news','politics','do_it_yourself','business']\n",
    "\n",
    "# Load the pickle files from coropus using CorpusLoader class\n",
    "loader = CorpusLoader(reader, 10, shuffle=True, categories=labels)\n",
    "\n",
    "models = []\n",
    "names = [LogisticRegression, SGDClassifier, MultinomialNB]\n",
    "for model in names:\n",
    "    models.append(pipeline(model()))\n",
    "\n",
    "def model_scores(models,loader):\n",
    "    for model in models:\n",
    "        name = model.named_steps['classifier'].__class__.__name__\n",
    "\n",
    "        scores = {\n",
    "        'model':str(model),\n",
    "        'name':name,\n",
    "        'accuracy':[],\n",
    "        'precision':[],\n",
    "        'recall':[],\n",
    "        'f1':[],\n",
    "        'time':[]\n",
    "        }\n",
    "\n",
    "        for X_train, X_test, y_train, y_test in loader:\n",
    "            start = time.time()\n",
    "            model.fit(X_train,y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            scores['time'].append(time.time() - start)\n",
    "            scores['accuracy'].append(accuracy_score(y_test, y_pred))\n",
    "            scores['precision'].append(precision_score(y_test, y_pred,average='weighted'))\n",
    "            scores['recall'].append(recall_score(y_test,y_pred,average='weighted'))\n",
    "            scores['f1'].append(f1_score(y_test,y_pred,average='weighted'))\n",
    "\n",
    "        scores['accuracy']=np.mean(scores['accuracy'])\n",
    "        scores['precision']=np.mean(scores['precision'])\n",
    "        scores['recall']=np.mean(scores['recall'])\n",
    "        scores['f1']=np.mean(scores['f1'])\n",
    "        yield scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model scores in json file.\n",
    "for scores in model_scores(models, loader):\n",
    "    with open('results.json','a') as f:\n",
    "        f.write(json.dumps(scores)+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's have a look at the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model                 precision    recall    accuracy        f1\n",
      "------------------  -----------  --------  ----------  --------\n",
      "SGDClassifier          0.678107  0.67647     0.678107  0.657237\n",
      "LogisticRegression     0.563833  0.587883    0.563833  0.472884\n",
      "MultinomialNB          0.45863   0.302512    0.45863   0.290672\n"
     ]
    }
   ],
   "source": [
    "headers= ['model','precision','recall','accuracy','f1']\n",
    "# Read result file\n",
    "df=pd.read_json('results.json',lines=True)\n",
    "df=df.drop(['model','time'],axis=1)\n",
    "\n",
    "table=list(df.values)\n",
    "table.sort(key=lambda r:r[-1], reverse=True)\n",
    "print(tabulate.tabulate(table,headers=headers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
